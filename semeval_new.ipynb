{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ivan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ivan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ivan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy \n",
    "import en_core_web_lg\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sklearn\n",
    "import time\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "from spacy import displacy\n",
    "import statistics\n",
    "\n",
    "nlp = en_core_web_lg.load()\n",
    "nlp_spacy = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import nltk\n",
    "import codecs\n",
    "from nltk.tokenize import PunktSentenceTokenizer,sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:07<00:00, 1389.74it/s]\n",
      "100%|██████████| 2021/2021 [00:00<00:00, 4748.34it/s]\n",
      "100%|██████████| 9999/9999 [00:00<00:00, 188500.21it/s]\n",
      "100%|██████████| 2020/2020 [00:00<00:00, 182325.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainingX = pd.read_csv('subtaskA_data_all.csv')\n",
    "trainingY = pd.read_csv('subtaskA_answers_all.csv')\n",
    "testX = pd.read_csv('data/taskA_trial_data.csv')\n",
    "testY = pd.read_csv('data/taskA_trial_answer.csv')\n",
    "pairsOfSentences=[]\n",
    "pairsOfSentences_test = []\n",
    "for i in tqdm(range(len(trainingX.values))):\n",
    "    pairsOfSentences.append([trainingX.values[i][1], trainingX.values[i][2]])\n",
    "                    \n",
    "for i in tqdm(range(len(testX.values))):\n",
    "    pairsOfSentences_test.append([testX.values[i][1], testX.values[i][2]])\n",
    "    \n",
    "whichOneIsWrong = [0]\n",
    "for i in tqdm(range (len(trainingY.values))):\n",
    "    whichOneIsWrong.append(trainingY.values[i][1])\n",
    "    \n",
    "whichOneIsWrong_test = [0]                   \n",
    "for i in tqdm(range(len(testY.values))):\n",
    "    whichOneIsWrong_test.append(testY.values[i][1])\n",
    "print(len(pairsOfSentences))\n",
    "print(len(whichOneIsWrong))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization_filter(sentence):\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = word_tokenize(sentence)\n",
    "    filtered_sent = \"\"\n",
    "    \n",
    "    for word in words:\n",
    "           if word not in stop_words:\n",
    "                    filtered_sent += (lemmatizer.lemmatize(word)) + \" \"\n",
    "\n",
    "    return filtered_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  First category filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_cat_filter(first_sent, second_sent):\n",
    "    first_sent = lemmatization_filter(first_sent)\n",
    "    second_sent = lemmatization_filter(second_sent)\n",
    "\n",
    "    tokens_first = nlp_spacy(first_sent)\n",
    "    tokens_second = nlp_spacy(second_sent)\n",
    "    \n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    \n",
    "    words_first = {}\n",
    "    words_second = {}\n",
    "  \n",
    "    \n",
    "    for i in range(len(tokens_first)):\n",
    "        if tokens_first[i].text.lower() not in stop_words:\n",
    "            words_first.update({i: lemmatizer.lemmatize(tokens_first[i].text.lower())})\n",
    "    for i in range(len(tokens_second)):\n",
    "        if tokens_second[i].text.lower() not in stop_words:\n",
    "            words_second.update({i: lemmatizer.lemmatize(tokens_second[i].text.lower())})\n",
    "        \n",
    "    first_sent_reduced = \"\"\n",
    "    second_sent_reduced = \"\"\n",
    "    \n",
    "    if len(words_second) == len(words_first):\n",
    "        for index, word in words_first.items():\n",
    "            if word in words_second.values():\n",
    "                if word != words_second.get(index):\n",
    "                    return False\n",
    "    else:            \n",
    "\n",
    "        for word in words_second.values():\n",
    "            if word in words_first.values():\n",
    "                second_sent_reduced += word\n",
    "\n",
    "        for word in words_first.values():\n",
    "            if word in words_second.values():\n",
    "                first_sent_reduced += word\n",
    "\n",
    "    \n",
    "        return lemmatization_filter(first_sent_reduced) == lemmatization_filter(second_sent_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dafa filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First category filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_cat_pairs = []\n",
    "first_cat_answers = []\n",
    "for i in tqdm(range(len(pairsOfSentences))):\n",
    "    if first_cat_filter(pairsOfSentences[i][0], pairsOfSentences[i][1]):\n",
    "        first_cat_pairs.append(pairsOfSentences[i])\n",
    "        first_cat_answers.append(whichOneIsWrong[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[0], y[0], X[0] -> training set\n",
    "# data[1], y[1], X[1] -> test set\n",
    "\n",
    "\n",
    "data = [None, None]\n",
    "y = [None, None]\n",
    "X = [None, None]\n",
    "\n",
    "data[0] = copy.deepcopy(pairsOfSentences)\n",
    "y[0] = copy.deepcopy(whichOneIsWrong)\n",
    "data[1] = copy.deepcopy(pairsOfSentences_test)\n",
    "y[1] = copy.deepcopy(whichOneIsWrong_test)\n",
    "\n",
    "# data[0] = copy.deepcopy(first_cat_pairs) \n",
    "# y[0] = copy.deepcopy(first_cat_answers)\n",
    "\n",
    "\n",
    "X[0] = [list() for i in range(len(data[0]))]\n",
    "X[1] = [list() for i in range(len(data[1]))]\n",
    "\n",
    "\n",
    "#print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X[0][0])\n",
    "# print(len(X[0]))\n",
    "# print(X[1][0])\n",
    "# print(len(X[1]))\n",
    "# for i in range(100):\n",
    "#     print(data[0][i][0])\n",
    "#     print(data[0][i][1])\n",
    "#     print(\"___________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Various functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words similarity with spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similarity_spacy_tokens(tokens_1, tokens_2):\n",
    "    # spacy tokens are used as input\n",
    "    sim = 0.0\n",
    "    count = 0\n",
    "    for tokenOuter in tokens_1:\n",
    "        for tokenInner in tokens_2:\n",
    "            sim += tokenOuter.similarity(tokenInner)\n",
    "            count += 1\n",
    "\n",
    "    if count != 0:\n",
    "        return sim / count\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "    #     if len(sim) != 0:\n",
    "#         return statistics.avg(sim)\n",
    "#     else:\n",
    "#         return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing sentences (removing subj, obj, pred, subj+obj, subj+pred, pred+obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_reduced_sentences(tokens, predicates, subjects, objects, forbidden):\n",
    "    without_sub = \"\"\n",
    "    without_obj = \"\"\n",
    "    without_pred = \"\"\n",
    "    without_sub_obj = \"\"\n",
    "    without_sub_pred = \"\"\n",
    "    without_pred_obj = \"\"\n",
    "    \n",
    "    for token in tokens:\n",
    "       # if token.text not in forbidden:\n",
    "        if token.text not in subjects:\n",
    "            without_sub += (token.text + \" \")\n",
    "            if token.text not in objects:\n",
    "                without_sub_obj += (token.text + \" \")\n",
    "            if token.text not in predicates:\n",
    "                without_sub_pred += (token.text + \" \")        \n",
    "        if token.text not in objects:\n",
    "            without_obj += (token.text + \" \")\n",
    "            if token.text not in predicates:\n",
    "                without_pred_obj += (token.text + \" \")\n",
    "        if token.text not in predicates:\n",
    "            without_pred += (token.text + \" \")\n",
    "    return [without_sub, without_obj, without_pred, without_sub_obj, without_sub_pred, without_pred_obj]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find subjects, objects and predicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_subj_obj_pred(spacy_tokens):\n",
    "    \n",
    "    SUBJECTS = [\"nsubj\", \"nsubjpass\", \"csubj\", \"csubjpass\", \"agent\", \"expl\"]\n",
    "    OBJECTS = [\"dobj\", \"dative\", \"attr\", \"oprd\"]\n",
    "    \n",
    "    subjects = []\n",
    "    objects = []\n",
    "    predicates = []\n",
    "    \n",
    "    for token in spacy_tokens:\n",
    "        if token.dep_ in SUBJECTS:\n",
    "            subjects.append(token)\n",
    "        elif token.dep_ in OBJECTS:\n",
    "            objects.append(token)\n",
    "        elif token.dep_ == \"ROOT\" or token.pos_ == \"VERB\":\n",
    "            predicates.append(token)\n",
    "    return subjects, objects, predicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pronouns to nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nounsToPronouns(sentence):\n",
    "    listOfWords = sentence.split()\n",
    "    for i in range(len(listOfWords)):\n",
    "        if listOfWords[i] == \"he\" or listOfWords[i] == \"him\":\n",
    "            listOfWords[i] = \"man\"\n",
    "        if listOfWords[i] == \"she\" or listOfWords[i] == \"her\":\n",
    "            listOfWords[i] = \"woman\"\n",
    "        if listOfWords[i] == \"they\" or listOfWords[i] == \"them\":\n",
    "            listOfWords[i] = \"persons\"\n",
    "        if listOfWords[i] == \"i\" or listOfWords[i] == \"me\":\n",
    "            listOfWords[i] = \"person\"\n",
    "        if listOfWords[i] == \"you\":\n",
    "            listOfWords[i] = \"person\"\n",
    "    return ' '.join(listOfWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract important parts of sentence as new sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_important_as_sent(sentence):\n",
    "    new_sentence = \"\"\n",
    "    tokens = extract_important_as_tokens(sentence)\n",
    "    for token in tokens:\n",
    "        new_sentence += token.text + \" \"\n",
    "    return new_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract important parts of sentence as spacy tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_important_as_tokens(sentence):\n",
    "    important_tokens = set()\n",
    "    parts = nlp_spacy(sentence)\n",
    "    for part in parts.noun_chunks:\n",
    "        important_tokens.add(part.root)\n",
    "        important_tokens.add(part.root.head)\n",
    "    return important_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity between words in sentence and hypernyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similarity_hypernyms(tokens_1, tokens_2):\n",
    "    sent_sim = -1\n",
    "                                    \n",
    "    for tokenOuter in tokens_1:\n",
    "        for tokenInner in tokens_2:\n",
    "            if len(wordnet.synsets(tokenOuter)) == 0:\n",
    "                break\n",
    "            else:\n",
    "                first_outer = (wordnet.synsets(tokenOuter))[0]\n",
    "            if len(wordnet.synsets(tokenInner)) == 0:\n",
    "                continue\n",
    "            else:\n",
    "                first_inner = (wordnet.synsets(tokenInner))[0]\n",
    "           \n",
    "            if first_outer.pos() == first_inner.pos() and tokenOuter != tokenInner: \n",
    "                for hypernym in first_inner.hypernyms(): \n",
    "                    sim = nlp_spacy(hypernym.name().split(\".\")[0]).similarity(nlp_spacy(first_outer.name().split(\".\")[0]))\n",
    "                    #sim = hypernym.wup_similarity(first_outer)\n",
    "                    if sim is not None:\n",
    "                        if sim > sent_sim:\n",
    "                            sent_sim = sim\n",
    "\n",
    "    return sent_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(find_similarity_hypernyms(lemmatization_filter(\"He drinks juice\"),\n",
    "                                [\"juice\"]))\n",
    "print(find_similarity_hypernyms(lemmatization_filter(\"He drinks apple\"),\n",
    "                                [\"apple\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentences manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing pronouns to nouns and lowering all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x_set in range \n",
    "for i in tqdm(range(len(data))):\n",
    "    data[x_set][i][0] = nounsToPronouns(data[x_set][i][0].lower())\n",
    "    data[x_set][i][1] = nounsToPronouns(data[x_set][i][1].lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing less important parts of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [02:02<00:00, 81.37it/s]\n",
      "100%|██████████| 2021/2021 [00:24<00:00, 81.13it/s]\n"
     ]
    }
   ],
   "source": [
    " for x_set in range(2):\n",
    "    for i in tqdm(range(len(data[x_set]))):\n",
    "        data[x_set][i][0] = extract_important_as_sent(data[x_set][i][0])\n",
    "        data[x_set][i][1] = extract_important_as_sent(data[x_set][i][1])\n",
    "    \n",
    "# for i in range(100):\n",
    "#     print(data[i][0])\n",
    "#     print(data[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector building models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tagging with relations (BAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)): \n",
    "    firstSentence = nlp(data[i][0])\n",
    "    secondSentence = nlp(data[i][1])\n",
    "    firstSentenceNumberOfRelations = 0\n",
    "    firstSentenceRelationSum = 0\n",
    "    secondSentenceNumberOfRelations = 0\n",
    "    secondSentenceRelationSum = 0 \n",
    "    for token in firstSentence:\n",
    "        for child in token.children:\n",
    "            firstSentenceNumberOfRelations += 1\n",
    "            firstSentenceRelationSum += token.similarity(child)\n",
    "    for token in secondSentence:\n",
    "        for child in token.children:\n",
    "            secondSentenceNumberOfRelations += 1\n",
    "            secondSentenceRelationSum += token.similarity(child)\n",
    "    firstSentenceAVG = firstSentenceRelationSum / firstSentenceNumberOfRelations\n",
    "    secondSentenceAVG = secondSentenceRelationSum / secondSentenceNumberOfRelations\n",
    "    X[i].append(firstSentenceAVG)\n",
    "    X[i].append(secondSentenceAVG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity of different words in between sentences with words in the same sentence - regular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [02:18<00:00, 72.02it/s]\n",
      "100%|██████████| 2021/2021 [00:26<00:00, 75.52it/s]\n"
     ]
    }
   ],
   "source": [
    "for x_set in range(2):\n",
    "    for i in tqdm(range(len(data[x_set]))):\n",
    "        firstSentence = data[x_set][i][0]\n",
    "        secondSentence = data[x_set][i][1]\n",
    "        tokensFirst = nlp_spacy(firstSentence)\n",
    "        tokensSecond = nlp_spacy(secondSentence)\n",
    "\n",
    "        firstSentDiffs = [] # words which are not in a second sentence\n",
    "        secondSentDiffs = [] # words which are not in a first sentence\n",
    "\n",
    "        for token in tokensFirst:\n",
    "            if token.text not in tokensSecond.text:\n",
    "                firstSentDiffs.append(token)\n",
    "        # print(firstSentDiffs)\n",
    "        for token in tokensSecond:\n",
    "            if token.text not in tokensFirst.text:\n",
    "                secondSentDiffs.append(token)\n",
    "\n",
    "        X[x_set][i].append(find_similarity_spacy_tokens(tokensFirst, firstSentDiffs))\n",
    "        X[x_set][i].append(find_similarity_spacy_tokens(tokensSecond, secondSentDiffs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity of different words in between sentences with words in the same sentence - lemmatization version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:46<00:00, 94.29it/s]\n",
      "100%|██████████| 2021/2021 [00:21<00:00, 95.00it/s]\n"
     ]
    }
   ],
   "source": [
    "for x_set in range(2):\n",
    "    for i in tqdm(range(len(data[x_set]))):\n",
    "        firstSentence = lemmatization_filter(data[x_set][i][0])\n",
    "        secondSentence = lemmatization_filter(data[x_set][i][1])\n",
    "        tokensFirst = nlp_spacy(firstSentence)\n",
    "        tokensSecond = nlp_spacy(secondSentence)\n",
    "\n",
    "        firstSentDiffs = [] # words which are not in a second sentence\n",
    "        secondSentDiffs = [] # words which are not in a first sentence\n",
    "\n",
    "        for token in tokensFirst:\n",
    "            if token.text not in tokensSecond.text:\n",
    "                firstSentDiffs.append(token)\n",
    "        # print(firstSentDiffs)\n",
    "        for token in tokensSecond:\n",
    "            if token.text not in tokensFirst.text:\n",
    "                secondSentDiffs.append(token)\n",
    "\n",
    "        X[x_set][i].append(find_similarity_spacy_tokens(tokensFirst, firstSentDiffs))\n",
    "        X[x_set][i].append(find_similarity_spacy_tokens(tokensSecond, secondSentDiffs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity of different words in between sentences with common words of two sentences - regular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:49<00:00, 91.03it/s]\n",
      "100%|██████████| 2021/2021 [00:22<00:00, 90.68it/s]\n"
     ]
    }
   ],
   "source": [
    "for x_set in range(2):    \n",
    "    for i in tqdm(range(len(data[x_set]))):\n",
    "        firstSentence = data[x_set][i][0]\n",
    "        secondSentence = data[x_set][i][1]\n",
    "        tokensFirst = nlp_spacy(firstSentence)\n",
    "        tokensSecond = nlp_spacy(secondSentence)\n",
    "        firstSentDiffs = [] # words which are not in a second sentence\n",
    "        secondSentDiffs = [] # words which are not in a first sentence\n",
    "        CommonWords = [] # words which are common to sentences\n",
    "        CommonTokens = [] # tokens which are common to sentences\n",
    "        for token in tokensFirst:\n",
    "            if token.text not in tokensSecond.text.split():\n",
    "                firstSentDiffs.append(token)\n",
    "            else:\n",
    "                if token.text not in CommonWords:\n",
    "                    CommonWords.append(token.text)\n",
    "                    CommonTokens.append(token)\n",
    "        #print(firstSentence)\n",
    "        #print(firstSentDiffs)\n",
    "        #print(CommonWords)\n",
    "        #print(CommonTokens)\n",
    "        for token in tokensSecond:\n",
    "            if token.text not in tokensFirst.text.split():\n",
    "                secondSentDiffs.append(token)\n",
    "            else:\n",
    "                if token.text not in CommonWords:\n",
    "                    CommonWords.append(token.text)\n",
    "                    CommonTokens.append(token)\n",
    "        firstDiffSimil = 0 # similarity of words that are not found in a second sentence with common words of sentences\n",
    "        secondDiffSimil = 0 # similarity of words that are not found in a first sentence with common words of sentences\n",
    "        for tokenOuter in CommonTokens:\n",
    "            for tokenInner in firstSentDiffs:\n",
    "                firstDiffSimil += tokenOuter.similarity(tokenInner)\n",
    "        for tokenOuter in CommonTokens:\n",
    "            for tokenInner in secondSentDiffs:\n",
    "                secondDiffSimil += tokenOuter.similarity(tokenInner)\n",
    "        if len(firstSentDiffs) != 0:\n",
    "            firstSentDiffAvg = firstDiffSimil / len(firstSentDiffs)\n",
    "            X[x_set][i].append(firstSentDiffAvg)\n",
    "        else:\n",
    "            X[x_set][i].append(0)\n",
    "        if len(secondSentDiffs) != 0:\n",
    "            secondSentDiffAvg = secondDiffSimil / len(secondSentDiffs)\n",
    "            X[x_set][i].append(secondSentDiffAvg)\n",
    "        else:\n",
    "            X[x_set][i].append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity of different words in between sentences with common words of two sentences - lemmatization version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:58<00:00, 84.11it/s]\n",
      "100%|██████████| 2021/2021 [00:23<00:00, 86.65it/s]\n"
     ]
    }
   ],
   "source": [
    "for x_set in range(2):    \n",
    "    for i in tqdm(range(len(data[x_set]))):\n",
    "        tokensFirst = nlp_spacy(lemmatization_filter(data[x_set][i][0]))\n",
    "        tokensSecond = nlp_spacy(lemmatization_filter(data[x_set][i][1]))\n",
    "        firstSentDiffs = [] # words which are not in a second sentence\n",
    "        secondSentDiffs = [] # words which are not in a first sentence\n",
    "        CommonWords = [] # words which are common to sentences\n",
    "        CommonTokens = [] # tokens which are common to sentences\n",
    "        for token in tokensFirst:\n",
    "            if token.text not in tokensSecond.text.split():\n",
    "                firstSentDiffs.append(token)\n",
    "            else:\n",
    "                if token.text not in CommonWords:\n",
    "                    CommonWords.append(token.text)\n",
    "                    CommonTokens.append(token)\n",
    "\n",
    "        for token in tokensSecond:\n",
    "            if token.text not in tokensFirst.text.split():\n",
    "                secondSentDiffs.append(token)\n",
    "            else:\n",
    "                if token.text not in CommonWords:\n",
    "                    CommonWords.append(token.text)\n",
    "                    CommonTokens.append(token)\n",
    "                    \n",
    "        firstDiffSimil = 0 # similarity of words that are not found in a second sentence with common words of sentences\n",
    "        secondDiffSimil = 0 # similarity of words that are not found in a first sentence with common words of sentences\n",
    "        \n",
    "        X[x_set][i].append(find_similarity_spacy_tokens(CommonTokens, firstSentDiffs))\n",
    "        X[x_set][i].append(find_similarity_spacy_tokens(CommonTokens, secondSentDiffs))\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity between predicate, subject and object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [02:07<00:00, 78.52it/s]\n",
      "100%|██████████| 2021/2021 [00:25<00:00, 78.32it/s]\n"
     ]
    }
   ],
   "source": [
    "for x_set in range(2):\n",
    "    for i in tqdm(range(len(data[x_set]))):\n",
    "        firstSentence = data[x_set][i][0]\n",
    "        secondSentence = data[x_set][i][1]\n",
    "\n",
    "        tokensFirst = nlp_spacy(firstSentence)\n",
    "        tokensSecond = nlp_spacy(secondSentence)\n",
    "\n",
    "        firstSubjects, firstObjects, firstPredicates = find_subj_obj_pred(tokensFirst)\n",
    "        secondSubjects, secondObjects, secondPredicates = find_subj_obj_pred(tokensSecond)\n",
    "\n",
    "        # vectors for similarity and number of vectors per category       \n",
    "        firstPS = find_similarity_spacy_tokens(firstPredicates, firstSubjects)\n",
    "        firstPO = find_similarity_spacy_tokens(firstPredicates, firstObjects)\n",
    "        firstSO = find_similarity_spacy_tokens(firstSubjects, firstObjects) \n",
    "        secondPS = find_similarity_spacy_tokens(secondPredicates, secondSubjects)\n",
    "        secondPO = find_similarity_spacy_tokens(secondPredicates, secondObjects) \n",
    "        secondSO = find_similarity_spacy_tokens(secondSubjects, secondObjects)  \n",
    "\n",
    "        X[x_set][i].append((firstPS + firstPO + firstSO) / 3)\n",
    "        X[x_set][i].append((secondPS + secondPO + secondSO) / 3)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity between predicate, subject and object - lemmatization version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:57<00:00, 85.46it/s]\n",
      "100%|██████████| 2021/2021 [00:23<00:00, 86.55it/s]\n"
     ]
    }
   ],
   "source": [
    "for x_set in range(2):\n",
    "    for i in tqdm(range(len(data[x_set]))):\n",
    "        \n",
    "        tokensFirst = nlp_spacy(lemmatization_filter(data[x_set][i][0]))\n",
    "        tokensSecond = nlp_spacy(lemmatization_filter(data[x_set][i][1]))\n",
    "\n",
    "        firstSubjects, firstObjects, firstPredicates = find_subj_obj_pred(tokensFirst)\n",
    "        secondSubjects, secondObjects, secondPredicates = find_subj_obj_pred(tokensSecond)\n",
    "\n",
    "        # vectors for similarity and number of vectors per category       \n",
    "        firstPS = find_similarity_spacy_tokens(firstPredicates, firstSubjects)\n",
    "        firstPO = find_similarity_spacy_tokens(firstPredicates, firstObjects)\n",
    "        firstSO = find_similarity_spacy_tokens(firstSubjects, firstObjects) \n",
    "        secondPS = find_similarity_spacy_tokens(secondPredicates, secondSubjects)\n",
    "        secondPO = find_similarity_spacy_tokens(secondPredicates, secondObjects) \n",
    "        secondSO = find_similarity_spacy_tokens(secondSubjects, secondObjects)  \n",
    "\n",
    "        X[x_set][i].append((firstPS + firstPO + firstSO) / 3)\n",
    "        X[x_set][i].append((secondPS + secondPO + secondSO) / 3)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity between subject and objects(ivan:reused,rino) NOT BAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(pairsOfSentences))):\n",
    "    firstSentence = pairsOfSentences[i][0]\n",
    "    secondSentence = pairsOfSentences[i][1]\n",
    "    tokensFirst = nlp_spacy(firstSentence)\n",
    "    tokensSecond = nlp_spacy(secondSentence)\n",
    "    \n",
    "    firstSubjects = []\n",
    "    firstObjects = []\n",
    "    firstPredicates = []\n",
    "    \n",
    "    secondSubjects = []\n",
    "    secondObjects = []\n",
    "    secondPredicates = []\n",
    "    \n",
    "    # getting predicates, subjects and objects out of sentences\n",
    "    for token in tokensFirst:\n",
    "        if token.dep_ == \"nsubj\":\n",
    "            firstSubjects.append(token)\n",
    "        elif token.dep_ == \"pobj\" or token.dep_ == \"nobj\" or token.dep_ == \"dobj\":\n",
    "            firstObjects.append(token)\n",
    "        elif token.dep_ == \"ROOT\" or token.dep_ == \"conj\":\n",
    "            firstPredicates.append(token)\n",
    "    for token in tokensSecond:\n",
    "        if token.dep_ == \"nsubj\":\n",
    "            secondSubjects.append(token)\n",
    "        elif token.dep_ == \"pobj\" or token.dep_ == \"nobj\" or token.dep_ == \"dobj\":\n",
    "            secondObjects.append(token)\n",
    "        elif token.dep_ == \"ROOT\" or token.dep_ == \"conj\":\n",
    "            secondPredicates.append(token)\n",
    "            \n",
    "    # vectors for similarity and number of vectors per category       \n",
    "    firstPO = 0 \n",
    "    fPoNum = len(firstPredicates) * len(firstObjects)\n",
    "    firstSO = 0 \n",
    "    fSoNum = len(firstSubjects) * len(firstObjects)\n",
    "    secondPO = 0 \n",
    "    sPoNum = len(secondPredicates) * len(secondObjects)\n",
    "    secondSO = 0\n",
    "    sSoNum = len(secondSubjects) * len(secondObjects)\n",
    "    \n",
    "    \n",
    "    for tokenP in firstPredicates:\n",
    "        for tokenO in firstObjects:\n",
    "            firstPO += (tokenP.similarity(tokenO))\n",
    "    for tokenS in firstSubjects:\n",
    "        for tokenO in firstObjects:\n",
    "            firstSO += (tokenS.similarity(tokenO))\n",
    "    \n",
    "    for tokenP in secondPredicates:\n",
    "        for tokenO in secondObjects:\n",
    "            secondPO += (tokenP.similarity(tokenO))\n",
    "    for tokenS in secondSubjects:\n",
    "        for tokenO in secondObjects:\n",
    "            secondSO +=(tokenS.similarity(tokenO))\n",
    "    \n",
    "    \n",
    "    \n",
    "    if (fPoNum + fSoNum) != 0:\n",
    "        avg = (firstPO + firstSO) / (fPoNum + fSoNum)\n",
    "        X[i].append(avg)\n",
    "    else:\n",
    "        X[i].append(0)\n",
    "        \n",
    "    \n",
    "    if (sPoNum + sSoNum) != 0:\n",
    "        avg = (secondPO + secondSO) / (sPoNum + sSoNum)\n",
    "        X[i].append(avg)\n",
    "    else:\n",
    "        X[i].append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity between predicate, subject and object (appended separately) (ivan:reused,rino) NOT BAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(pairsOfSentences))):\n",
    "    firstSentence = pairsOfSentences[i][0]\n",
    "    secondSentence = pairsOfSentences[i][1]\n",
    "    tokensFirst = nlp_spacy(firstSentence)\n",
    "    tokensSecond = nlp_spacy(secondSentence)\n",
    "    \n",
    "    firstSubjects = []\n",
    "    firstObjects = []\n",
    "    firstPredicates = []\n",
    "    \n",
    "    secondSubjects = []\n",
    "    secondObjects = []\n",
    "    secondPredicates = []\n",
    "    \n",
    "    # getting predicates, subjects and objects out of sentences\n",
    "    for token in tokensFirst:\n",
    "        if token.dep_ == \"nsubj\":\n",
    "            firstSubjects.append(token)\n",
    "        elif token.dep_ == \"pobj\" or token.dep_ == \"nobj\" or token.dep_ == \"dobj\":\n",
    "            firstObjects.append(token)\n",
    "        elif token.dep_ == \"ROOT\" or token.dep_ == \"conj\":\n",
    "            firstPredicates.append(token)\n",
    "    for token in tokensSecond:\n",
    "        if token.dep_ == \"nsubj\":\n",
    "            secondSubjects.append(token)\n",
    "        elif token.dep_ == \"pobj\" or token.dep_ == \"nobj\" or token.dep_ == \"dobj\":\n",
    "            secondObjects.append(token)\n",
    "        elif token.dep_ == \"ROOT\" or token.dep_ == \"conj\":\n",
    "            secondPredicates.append(token)\n",
    "            \n",
    "    # vectors for similarity and number of vectors per category       \n",
    "    firstPS = 0 \n",
    "    fPsNum = len(firstPredicates) * len(firstSubjects)\n",
    "    firstPO = 0 \n",
    "    fPoNum = len(firstPredicates) * len(firstObjects)\n",
    "    firstSO = 0 \n",
    "    fSoNum = len(firstSubjects) * len(firstObjects)\n",
    "    secondPS = 0\n",
    "    sPsNum = len(secondPredicates) * len(secondSubjects)\n",
    "    secondPO = 0 \n",
    "    sPoNum = len(secondPredicates) * len(secondObjects)\n",
    "    secondSO = 0\n",
    "    sSoNum = len(secondSubjects) * len(secondObjects)\n",
    "    \n",
    "    \n",
    "    for tokenP in firstPredicates:\n",
    "        for tokenS in firstSubjects:\n",
    "            firstPS += (tokenP.similarity(tokenS))\n",
    "    for tokenP in firstPredicates:\n",
    "        for tokenO in firstObjects:\n",
    "            firstPO += (tokenP.similarity(tokenO))\n",
    "    for tokenS in firstSubjects:\n",
    "        for tokenO in firstObjects:\n",
    "            firstSO += (tokenS.similarity(tokenO))\n",
    "    \n",
    "    for tokenP in secondPredicates:\n",
    "        for tokenS in secondSubjects:\n",
    "            secondPS += (tokenP.similarity(tokenS))\n",
    "    for tokenP in secondPredicates:\n",
    "        for tokenO in secondObjects:\n",
    "            secondPO += (tokenP.similarity(tokenO))\n",
    "    for tokenS in secondSubjects:\n",
    "        for tokenO in secondObjects:\n",
    "            secondSO +=(tokenS.similarity(tokenO))\n",
    "    \n",
    "    #FIRST SENTENCE\n",
    "    \n",
    "    if fPsNum > 0:\n",
    "        X[i].append(firstPS/fPsNum)\n",
    "    else:\n",
    "        X[i].append(0)\n",
    "        \n",
    "    if fPoNum > 0:\n",
    "        X[i].append(firstPO/fPoNum)\n",
    "    else:\n",
    "        X[i].append(0)\n",
    "\n",
    "    if fSoNum > 0:\n",
    "        X[i].append(firstSO/fSoNum)\n",
    "    else:\n",
    "        X[i].append(0)\n",
    "        \n",
    "    #SECONDSENTENCE\n",
    "    \n",
    "    if sPsNum > 0:\n",
    "        X[i].append(secondPS/sPsNum)\n",
    "    else:\n",
    "        X[i].append(0)\n",
    "    \n",
    "    if sPoNum > 0:\n",
    "        X[i].append(secondPO/sPoNum)\n",
    "    else:\n",
    "        X[i].append(0)\n",
    "    \n",
    "    if sSoNum > 0:\n",
    "        X[i].append(secondSO/sSoNum)\n",
    "    else:\n",
    "        X[i].append(0)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity of disctinct words and hypernyms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [list() for i in range(len(data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(data))):\n",
    "    firstSentence = data[i][0]\n",
    "    secondSentence = data[i][1]\n",
    "    \n",
    "    tokensFirst = nlp_spacy(firstSentence)\n",
    "    tokensSecond = nlp_spacy(secondSentence)\n",
    "    \n",
    "    firstSentDiffs = [] # words which are not in a second sentence\n",
    "    secondSentDiffs = [] # words which are not in a first sentence\n",
    "  \n",
    "    for token in tokensFirst:\n",
    "        if token.text not in tokensSecond.text:\n",
    "            firstSentDiffs.append(token.text)\n",
    "\n",
    "    for token in tokensSecond:\n",
    "        if token.text not in tokensFirst.text:\n",
    "            secondSentDiffs.append(token.text)\n",
    "    \n",
    "    X[i].append(find_similarity_hypernyms(lemmatization_filter(firstSentence), secondSentDiffs))\n",
    "    X[i].append(find_similarity_hypernyms(lemmatization_filter(secondSentence), firstSentDiffs))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual cross validation with SVC(rbf) model and wrong predictions extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "classifier = SVC(kernel = 'rbf', random_state = 53)\n",
    "\n",
    "\n",
    "number_of_cross_validations = 10\n",
    "X_parts = [list() for i in range (number_of_cross_validations)]\n",
    "y_parts = [list() for i in range (number_of_cross_validations)]\n",
    "\n",
    "\n",
    "# spliting data in number_of_cross_validations training sets and test sets\n",
    "for i in range(len(X)):\n",
    "    X_parts[i % number_of_cross_validations].append(X[i])\n",
    "for i in range(len(X)):\n",
    "    y_parts[i % number_of_cross_validations].append(y[i])\n",
    "\n",
    "\n",
    "X_training_set = [list() for i in range (number_of_cross_validations)]\n",
    "for i in range(number_of_cross_validations):\n",
    "    for j in range(number_of_cross_validations):\n",
    "        if j != i:\n",
    "            X_training_set[i].extend(X_parts[j])\n",
    "            \n",
    "            \n",
    "y_training_set = [list() for i in range (number_of_cross_validations)]\n",
    "for i in range(number_of_cross_validations):\n",
    "    for j in range(number_of_cross_validations):\n",
    "        if j != i:\n",
    "            y_training_set[i].extend(y_parts[j])\n",
    "\n",
    "            \n",
    "\n",
    "wrong_sentence_pairs = [list() for i in range (number_of_cross_validations)] # stores every pair that is wrongly predicted\n",
    "wrong_sent_answers = [list() for i in range (number_of_cross_validations)] # which of 2 sentences is wrong\n",
    "\n",
    "\n",
    "sc = StandardScaler()\n",
    "test_sum = 0\n",
    "\n",
    "# performing cross validation using linear regression\n",
    "for i in range(number_of_cross_validations):\n",
    "    \n",
    "    X_train = sc.fit_transform(X_training_set[i])\n",
    "    X_test = sc.transform(X_parts[i])\n",
    "    \n",
    "    classifier.fit(X_train, y_training_set[i])\n",
    "    \n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    for j in range(len(y_pred)):\n",
    "        if y_pred[j] != y_parts[i][j]:\n",
    "            wrong_sentence_pairs[i].append(pairsOfSentences[i + 10 * j])\n",
    "            wrong_sent_answers[i].append(y_parts[i][j])\n",
    "    \n",
    "    cm = confusion_matrix(y_parts[i], y_pred)\n",
    "    true = cm[0][0]+cm[1][1]\n",
    "    false = cm[0][1]+cm[1][0]\n",
    "    \n",
    "    test_sum += (true / (true + false))\n",
    "    \n",
    "    print(\"Accuracy in run number \" + str(i + 1) + \" is: \" +  str(true / (true + false)))\n",
    "    \n",
    "print(\"Avg accuracy is: \", test_sum / number_of_cross_validations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM-linear: Accuracy: 0.70 (+/- 0.08)\n"
     ]
    }
   ],
   "source": [
    "print(\"SVM-linear: \" + svc_linear(X[0], y[0], 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM-rbf: Accuracy: 0.70 (+/- 0.09)\n"
     ]
    }
   ],
   "source": [
    "print(\"SVM-rbf: \" + svc_rbf(X[0] + X[1], y[0] + y[1], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression: Accuracy: 0.66 (+/- 0.09)\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic regression: \" + logistic_regression(X, y, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest: Accuracy: 0.65 (+/- 0.06)\n"
     ]
    }
   ],
   "source": [
    "print(\"Random forest: \" + random_forest(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM-linear (66%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svc_linear(X, y, c):\n",
    "    clf = svm.SVC(kernel='linear', C=c)\n",
    "    scores = cross_val_score(clf, X, y, cv=10)\n",
    "    return(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression (67%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X, y, rs):\n",
    "    scaler = StandardScaler()\n",
    "    #X = scaler.fit_transform(X)\n",
    "    classifier = LogisticRegression(random_state = rs, max_iter=2000)\n",
    "    scores = cross_val_score(classifier, X, y, cv=10)\n",
    "    return(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM-rbf (67%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svc_rbf(X, y, rs):\n",
    "    classifier = SVC(kernel = 'rbf', random_state = rs)\n",
    "    scores = cross_val_score(classifier, X, y, cv=10)\n",
    "    return(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(X, y):\n",
    "    classifier = RandomForestClassifier(n_estimators = 100, criterion = 'entropy', random_state = 0)\n",
    "    scores = cross_val_score(classifier, X, y, cv=10)\n",
    "    return(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4629705523451169, 0.4936219722032547, 0.3935041083022952, 0.40340409108570646, 0.29969875141978264, 0.3670274652540684, 0.9438835829496384, 0.9514634708563486]\n",
      "[0.4629705523451169, 0.4936219722032547, 0.3935041083022952, 0.40340409108570646, 0.29969875141978264, 0.3670274652540684, 0.9438835829496384, 0.9514634708563486]\n"
     ]
    }
   ],
   "source": [
    "X_moj = copy.deepcopy(X)\n",
    "#X = copy.deepcopy(X_moj)\n",
    "print(X[0][0])\n",
    "print(X_moj[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.7570509648688768\n"
     ]
    }
   ],
   "source": [
    "#best -> 6.2 + 6.3 + 6.4 + 6.5 + 6. 6 + 6.3(reduced) -> 76.79 %\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "classifier1 = svm.SVC(kernel='linear', C=2)\n",
    "classifier2 = RandomForestClassifier(n_estimators = 100, criterion = 'entropy', random_state = 0)\n",
    "classifier3 = SVC(kernel = 'rbf', random_state = 10)\n",
    "classifier4 = LogisticRegression(random_state = 10, max_iter=2000)\n",
    "\n",
    "classifier2.fit(X[0], y[0])\n",
    "y_pred = classifier2.predict(X[1])\n",
    "\n",
    "cm = confusion_matrix(y[1], y_pred)\n",
    "true = cm[0][0]+cm[1][1]\n",
    "false = cm[0][1]+cm[1][0]\n",
    "print(\"Accuracy\", true/(true + false)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "848.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
